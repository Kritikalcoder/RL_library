{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Imports.ipynb\n",
    "%run Discrete_Agent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent_rsa(DiscreteAgent):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.is_stable = False\n",
    "        self.values = {}\n",
    "        self.policy = {}\n",
    "        self.delta = 1.0\n",
    "        self.theta = 0.1\n",
    "        \n",
    "        self.sweep_no = 0\n",
    "        self.is_converged = False\n",
    "        self.max_sweeps = 1000\n",
    "        \n",
    "        for state in self.env.states:\n",
    "            self.values[state] = 0\n",
    "            actions_list = self.env.actions[state]\n",
    "            self.policy[state] = random.choice(actions_list)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = self.policy[state]\n",
    "        return action\n",
    "    \n",
    "    def evaluate_policy(self):\n",
    "        self.is_converged = False\n",
    "        while self.delta >= self.theta and self.sweep_no < self.max_sweeps:\n",
    "            self.sweep_no += 1\n",
    "            self.delta = 0\n",
    "            for state in self.env.states:\n",
    "                val = self.values[state]\n",
    "                act = self.policy[state]\n",
    "                val_sum = self.env.rewards[(state,act)]\n",
    "                for dest in self.env.states:\n",
    "                    if (state,act,dest) in self.env.transitions:\n",
    "                        val_sum += (self.env.gamma * self.values[dest]) * self.env.transitions[(state,act,dest)]\n",
    "                self.values[state] = copy.deepcopy(val_sum)\n",
    "                self.delta = max(self.delta, abs(val - val_sum))\n",
    "                \n",
    "        if self.delta < self.theta:\n",
    "            self.is_converged = True\n",
    "                    \n",
    "    def improve_policy(self): \n",
    "        self.is_stable = True\n",
    "        for state in self.env.states:\n",
    "            old_act = self.policy[state]\n",
    "            val_max = self.values[state]\n",
    "            act_max = self.policy[state]\n",
    "            \n",
    "            for act in self.env.actions[state]:\n",
    "                val_sum = self.env.rewards[(state,act)]\n",
    "                \n",
    "                for dest in self.env.states:\n",
    "                    if (state,act,dest) in self.env.transitions:\n",
    "                        val_sum += (self.env.gamma * self.values[dest]) * self.env.transitions[(state,act,dest)]\n",
    "                        \n",
    "                if val_sum > val_max:\n",
    "                    val_max = val_sum\n",
    "                    act_max = act   \n",
    "                elif val_sum == val_max:\n",
    "                    act_max = random.choice([act_max, act])\n",
    "            \n",
    "            self.values[state] = copy.deepcopy(val_max)\n",
    "            self.policy[state] = copy.deepcopy(act_max)\n",
    "            if old_act != act_max:\n",
    "                self.is_stable = False\n",
    "                \n",
    "    def update(self):\n",
    "        # Evaluate + Update\n",
    "        self.sweep_no = 0\n",
    "        while not self.is_stable:\n",
    "            self.evaluate_policy()\n",
    "            self.improve_policy()\n",
    "        return self.sweep_no, self.is_converged, self.is_stable, self.values, self.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent_rsas(DiscreteAgent):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.is_stable = False\n",
    "        self.values = {}\n",
    "        self.policy = {}\n",
    "        self.delta = 1.0\n",
    "        self.theta = 0.1\n",
    "        \n",
    "        self.sweep_no = 0\n",
    "        self.is_converged = False\n",
    "        self.max_sweeps = 1000\n",
    "        \n",
    "        for state in self.env.states:\n",
    "            self.values[state] = 0\n",
    "            actions_list = self.env.actions[state]\n",
    "            self.policy[state] = random.choice(actions_list)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = self.policy[state]\n",
    "        return action\n",
    "    \n",
    "    def evaluate_policy(self):\n",
    "        self.is_converged = False\n",
    "        while self.delta >= self.theta and self.sweep_no < self.max_sweeps:\n",
    "            self.sweep_no += 1\n",
    "            self.delta = 0\n",
    "            for state in self.env.states:\n",
    "                val = self.values[state]\n",
    "                act = self.policy[state]\n",
    "#                 val_sum = self.env.rewards[(state,act)]\n",
    "                val_sum = 0.0\n",
    "                for dest in self.env.states:\n",
    "                    if (state,act,dest) in self.env.transitions:\n",
    "                        val_sum += (self.env.gamma * self.values[dest]) * self.env.transitions[(state,act,dest)]\n",
    "                        val_sum += self.env.rewards[(state,act,dest)] * self.env.transitions[(state,act,dest)]\n",
    "                self.values[state] = copy.deepcopy(val_sum)\n",
    "                self.delta = max(self.delta, abs(val - val_sum))\n",
    "                \n",
    "        if self.delta < self.theta:\n",
    "            self.is_converged = True\n",
    "                    \n",
    "    def improve_policy(self): \n",
    "        self.is_stable = True\n",
    "        for state in self.env.states:\n",
    "            old_act = self.policy[state]\n",
    "            val_max = self.values[state]\n",
    "            act_max = self.policy[state]\n",
    "            \n",
    "            for act in self.env.actions[state]:\n",
    "#                 val_sum = self.env.rewards[(state,act)]\n",
    "                val_sum = 0.0\n",
    "                for dest in self.env.states:\n",
    "                    if (state,act,dest) in self.env.transitions:\n",
    "                        val_sum += (self.env.gamma * self.values[dest]) * self.env.transitions[(state,act,dest)]\n",
    "                        val_sum += self.env.rewards[(state,act,dest)] * self.env.transitions[(state,act,dest)]\n",
    "                if val_sum > val_max:\n",
    "                    val_max = val_sum\n",
    "                    act_max = act   \n",
    "                elif val_sum == val_max:\n",
    "                    act_max = random.choice([act_max, act])\n",
    "            \n",
    "            self.values[state] = copy.deepcopy(val_max)\n",
    "            self.policy[state] = copy.deepcopy(act_max)\n",
    "            if old_act != act_max:\n",
    "                self.is_stable = False\n",
    "                \n",
    "    def update(self):\n",
    "        # Evaluate + Update\n",
    "        self.sweep_no = 0\n",
    "        while not self.is_stable:\n",
    "            self.evaluate_policy()\n",
    "            self.improve_policy()\n",
    "        return self.sweep_no, self.is_converged, self.is_stable, self.values, self.policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
