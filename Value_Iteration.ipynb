{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Imports.ipynb\n",
    "%run Discrete_Agent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent_rsa(DiscreteAgent):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.values = {}\n",
    "        self.delta = 1.0\n",
    "        self.theta = 0.01\n",
    "        self.policy = {}\n",
    "        \n",
    "        self.sweep_no = 0\n",
    "        self.is_converged = False\n",
    "        self.max_sweeps = 1000\n",
    "        \n",
    "        for state in self.env.states:\n",
    "            self.values[state] = 0\n",
    "            actions_list = self.env.actions[state]\n",
    "            self.policy[state] = random.choice(actions_list)\n",
    "        \n",
    "    def update(self):\n",
    "        self.sweep_no = 0\n",
    "        while self.delta >= self.theta and self.sweep_no < self.max_sweeps:\n",
    "            self.sweep_no += 1\n",
    "            self.delta = 0\n",
    "            for state in self.env.states:\n",
    "                val = self.values[state]\n",
    "                val_max = self.values[state]\n",
    "                act_max = self.policy[state]\n",
    "\n",
    "                for act in self.env.actions[state]:\n",
    "                    val_sum = self.env.rewards[(state,act)]\n",
    "\n",
    "                    for dest in self.env.states:\n",
    "                        if (state,act,dest) in self.env.transitions:\n",
    "                            val_sum += (self.env.gamma * self.values[dest]) * self.env.transitions[(state,act,dest)]\n",
    "\n",
    "                    if val_sum > val_max:\n",
    "                        val_max = val_sum\n",
    "                        act_max = act   \n",
    "                    elif val_sum == val_max:\n",
    "                        # issue with floats\n",
    "                        act_max = random.choice([act_max, act])\n",
    "\n",
    "                self.values[state] = copy.deepcopy(val_max)\n",
    "                self.policy[state] = copy.deepcopy(act_max)\n",
    "                \n",
    "                self.delta = max(self.delta, abs(val - val_max))\n",
    "        \n",
    "        if self.delta < self.theta:\n",
    "            self.is_converged = True\n",
    "            \n",
    "        # calculated optimal deterministic policy\n",
    "        \n",
    "        return self.sweep_no, self.is_converged, self.values, self.policy\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = self.policy[state]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent_rsas(DiscreteAgent):\n",
    "    \n",
    "    def __init__(self, env, theta):\n",
    "        \n",
    "        self.env = env\n",
    "        self.values = {}\n",
    "        self.delta = 1.0\n",
    "        self.theta = theta\n",
    "        self.policy = {}\n",
    "        \n",
    "        self.sweep_no = 0\n",
    "        self.is_converged = False\n",
    "        self.max_sweeps = 1000\n",
    "        \n",
    "        for state in self.env.states:\n",
    "            self.values[state] = 0\n",
    "            actions_list = self.env.actions[state]\n",
    "            self.policy[state] = random.choice(actions_list)\n",
    "        \n",
    "    def update(self):\n",
    "        self.sweep_no = 0\n",
    "        while self.delta >= self.theta and self.sweep_no < self.max_sweeps:\n",
    "            self.sweep_no += 1\n",
    "            self.delta = 0\n",
    "            for state in self.env.states:\n",
    "                val = self.values[state]\n",
    "                val_max = self.values[state]\n",
    "                act_max = self.policy[state]\n",
    "\n",
    "                for act in self.env.actions[state]:\n",
    "                    val_sum = 0\n",
    "                    for dest in self.env.states:\n",
    "                        if (state,act,dest) in self.env.transitions:\n",
    "                            val_sum += (self.env.gamma * self.values[dest]) * self.env.transitions[(state,act,dest)]\n",
    "                            val_sum += self.env.rewards[(state,act,dest)] * self.env.transitions[(state,act,dest)]\n",
    "#                     if act == 100 - state:\n",
    "#                         val_sum += self.env.bias * (self.values[100] + 1000)\n",
    "#                         val_sum += (1 - self.env.bias) * (self.values[state-act])\n",
    "#                     elif state == 100 + act:\n",
    "#                         val_sum += self.env.bias * (self.values[state + act])\n",
    "#                         val_sum += (1 - self.env.bias) * (self.values[100] + 1000)\n",
    "#                     else:\n",
    "#                         val_sum += (self.env.bias * (self.values[state + act])) + ((1 - self.env.bias) * (self.values[state-act]))\n",
    "#                     if val_sum > val_max:\n",
    "#                         val_max = val_sum\n",
    "#                         act_max = act   \n",
    "#                     elif val_sum == val_max:\n",
    "#                         # issue with floats\n",
    "#                         act_max = random.choice([act_max, act])\n",
    "\n",
    "                self.values[state] = copy.deepcopy(val_max)\n",
    "                self.policy[state] = copy.deepcopy(act_max)\n",
    "                \n",
    "                self.delta = max(self.delta, abs(val - val_max))\n",
    "                \n",
    "        if self.delta < self.theta:\n",
    "#             print(\"Converged\")\n",
    "            self.is_converged = True\n",
    "            \n",
    "        # calculated optimal deterministic policy\n",
    "        \n",
    "        return self.sweep_no, self.is_converged, self.values, self.policy\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = self.policy[state]\n",
    "        return action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
