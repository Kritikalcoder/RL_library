{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Imports.ipynb\n",
    "%run Discrete_Env.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Rental Environment\n",
    "\n",
    "### State Space $S$\n",
    "A state in this MDP comprises of the number of cars at each location at the end of the day. This is equivalent to the number of cars available for renting the next day prior to movement of cars from one location to another.  \n",
    "State Space $S$ consists of tuples of the form (no. of cars at $Loc_1$ and no. of cars at $Loc_2$) at the end of each day (time step $t$).  \n",
    "Let:  \n",
    "no. of cars at $Loc_1 = C1$  \n",
    "no. of cars at $Loc_2 = C2$  \n",
    "Then, $S = \\{(C1, C2) \\ | \\ 0 \\leq C1, C2 \\leq 20; C1, C2 \\in \\mathbb{Z}\\}$  \n",
    "Hence, $|S| = 21 * 21 = 441$ states  \n",
    "\n",
    "### Action Space $A$\n",
    "The action performed is moving cars from $Loc_1$ to $Loc_2$ overnight. It is represented by the number of cars moved. A negative number indicates that those many cars are moved from $Loc_2$ to $Loc_1$ overnight.  \n",
    "Action Space $A = \\{-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5\\}$ over all states in $S$.  \n",
    "$|A| = 11$  \n",
    "For a given state $s = (C1, C2) \\in S$,   \n",
    "$A(s) = \\{\\max (-C2, -5) \\leq x \\leq \\min (C1, 5) \\ | \\ x \\in \\mathbb{Z}\\}$  \n",
    "Note that $(C1 - x)$, or $(C2 + x)$ can exceed $20$ (as mentioned in the problem).  \n",
    "\n",
    "### Flow from $(s(t)$, $a(t))$ to $s(t+1)$\n",
    "$s(t) = (C1, C2)$ number of cars available for renting at night of day $t-1$ (for day $t$)  \n",
    "$a(t) = x$ number of cars moved from $Loc_1$ to $Loc_2$ at night of day $t-1$  \n",
    "$\\max (-C2, -5) \\leq x \\leq \\min (C1, 5)$  \n",
    "Internally:    \n",
    "- Now, $(\\min (C1 - x, 20), \\min (C2 + x, 20))$ number of cars available for renting for day $t$   \n",
    "- Renting on day $t$:   \n",
    "$A1$ number of cars rented from $Loc_1 \\ (0 \\leq A1 \\leq \\min (C1 - x, 20))$  \n",
    "$A2$ number of cars rented from $Loc_2 \\ (0 \\leq A2 \\leq \\min (C2 + x, 20))$  \n",
    "Since these numbers are generated from Poisson disributions,  \n",
    "$0 \\leq A1 = \\min (Poisson (\\lambda = 3), \\min (C1 - x, 20))$  \n",
    "$0 \\leq A2 = \\min (Poisson (\\lambda = 4), \\min (C2 + x, 20))$  \n",
    "- Now, $(C1 - x -A1, C2 + x - A2)$ number of cars available for renting for day $t$, but renting is done  \n",
    "- Returning on day $t$ (available for renting for day $t+1$):  \n",
    "$B1$ number of cars returned to $Loc_1 \\ (0 \\leq B1 \\leq 20 - \\min (C1 - x, 20) + A1)$   \n",
    "$B2$ number of cars returned to $Loc_2 \\ (0 \\leq B2 \\leq 20 - \\min (C2 + x, 20) + A2)$  \n",
    "Since these numbers are generated from Poisson disributions,  \n",
    "$0 \\leq B1 = \\min (Poisson (\\lambda=3), 20 - \\min (C1 - x, 20) + A1)$  \n",
    "$0 \\leq B2 = \\min (Poisson (\\lambda=2), 20 - \\min (C2 + x, 20) + A2)$  \n",
    "- Now, $(\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$ number of cars available for renting for day $t$  \n",
    "  \n",
    "Then $s(t+1) = (\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$ number of cars available for renting at night of day $t$ (for day $t+1$).  \n",
    "\n",
    "### Transition Probability Function $p(s'|s,a)$\n",
    "The transition probability function for taking action $a(t)$ from state $s(t)$ to reach state $s(t+1)$ is described as follows:  \n",
    "Let $s(t) = (C1, C2)$  \n",
    "$a(t) = x$    \n",
    "$\\max (-C2 ,-5) \\leq x \\leq \\min (C1, 5)$  \n",
    "Then $s(t+1) = (\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$,  \n",
    "where,  \n",
    "$0 \\leq A1 = \\min (Poisson (\\lambda=3), \\min (C1 - x, 20))$  \n",
    "$0 \\leq A2 = \\min (Poisson (\\lambda=4), \\min (C2 + x, 20))$  \n",
    "$0 \\leq B1 = \\min (Poisson (\\lambda=3), 20 - \\min (C1 - x, 20) + A1)$  \n",
    "$0 \\leq B2 = \\min (Poisson (\\lambda=2), 20 - \\min (C2 + x, 20) + A2)$  \n",
    "  \n",
    "Probability of no. of cars rented from $Loc_1$ being $A1$:  \n",
    "$P1 (A1)$  \n",
    "$= Poisson (A1 \\ | \\ \\lambda = 3), if \\ A1 < \\min (C1 - x, 20)$  \n",
    "$= \\sum_{i = A1}^{\\infty} Poisson (i \\ | \\ \\lambda = 3), if \\ A1 = \\min (C1 - x, 20)$  \n",
    "  \n",
    "Probability of no. of cars rented from $Loc_2$ being $A2$:  \n",
    "$P2 (A2)$  \n",
    "$= Poisson (A2 \\ | \\ \\lambda = 4), if \\ A2 < \\min (C2 + x, 20)$  \n",
    "$= \\sum_{i = A2}^{\\infty} Poisson (i \\ | \\ \\lambda = 4), if \\ A2 = \\min (C2 + x, 20)$  \n",
    "  \n",
    "Probability of no. of cars returned to $Loc_1$ being $B1$:  \n",
    "$Q1 (B1)$  \n",
    "$= Poisson (B1 \\ | \\ \\lambda = 3), if \\ B1 < 20 - \\min (C1 - x, 20) + A1$  \n",
    "$= \\sum_{i = B1}^{\\infty} Poisson (i \\ | \\ \\lambda = 3), if \\ B1 = 20 - \\min (C1 - x, 20) + A1$  \n",
    "  \n",
    "Probability of no. of cars returned to $Loc_2$ being $B2$:  \n",
    "$Q2 (B2)$  \n",
    "$= Poisson (B2 \\ | \\ \\lambda = 2), if \\ B2 < 20 - \\min (C2 + x, 20) + A2$  \n",
    "$= \\sum_{i = B2}^{\\infty} Poisson (i \\ | \\ \\lambda = 2), if \\ B2 = 20 - \\min (C2 + x, 20) + A2$  \n",
    "  \n",
    "Thus, $\\ p(s(t+1) \\ | \\ s(t), a(t)) = P1 (A1) * P2 (A2) * Q1 (B1) * Q2 (B2)$  \n",
    "  \n",
    "### Reward Function $R(s,a,s')$\n",
    "The reward obtained for taking action $a(t)$ from state $s(t)$ to reach state $s(t+1)$ is described as follows:  \n",
    "Let   \n",
    "$s(t) = (C1, C2)$,    \n",
    "$a(t) = x$, and   \n",
    "$s(t+1) = (\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$  \n",
    "(as defined above)  \n",
    "Then, $R(s(t), a(t), s(t + 1)) =  (10 * (A1 + A2)) - (2 * |x|)$  \n",
    "  \n",
    "### Discount Rate $\\gamma$  \n",
    "Discount rate for this MDP is defined to be $\\gamma = 0.9 \\ (0 <= \\gamma <= 1)$.  \n",
    "\n",
    "## Bellman Updates for Policy Iteration\n",
    "\n",
    "### Policy Evaluation step\n",
    "$\\forall \\ s \\in S$:  \n",
    "$V(s) = \\sum_{s'} p(s'|s,\\pi(s))[r(s,\\pi(s),s') + \\gamma \\cdot V(s')]$  \n",
    "(Bellman Expectation Equation)  \n",
    "For our domain, let  \n",
    "$s = (C1, C2)$  \n",
    "$\\pi(s) = x$  \n",
    "$s' = (\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$  \n",
    "$\\ p(s' \\ | \\ s, \\pi(s) = x) = P1 (A1) * P2 (A2) * Q1 (B1) * Q2 (B2)$  \n",
    "$R(s, \\pi(s) = x, s') =  (10 * (A1 + A2)) - (2 * |x|)$  \n",
    "$\\gamma = 0.9$  \n",
    "\n",
    "Then the Bellman Expectation Equation becomes:  \n",
    "$V(C1,C2) = \\sum_{s'}  P1 (A1) . P2 (A2) . Q1 (B1) . Q2 (B2) . [(10(A1 + A2)) - (2|x|) + 0.9 V(s')]$  \n",
    "where $s' =  (\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$  \n",
    "  \n",
    "### Policy Improvement Step\n",
    "$\\forall \\ s \\in S$:  \n",
    "$\\pi(s) = \\arg\\max_{a} \\sum_{s'} p(s'|s,a)[r(s,a,s') + \\gamma \\cdot V(s')]$  \n",
    "(Bellman Optimality Equation)  \n",
    "\n",
    "Similarly, we can formulate the Bellman Optimality Equation to get the improved policy:  \n",
    "$\\pi(C1,C2) = \\arg\\max_{x} \\sum_{s'}  P1 (A1) . P2 (A2) . Q1 (B1) . Q2 (B2) . [(10(A1 + A2)) - (2|x|) + 0.9 V(s')]$  \n",
    "where $s' =  (\\min (C1 - x, 20) - A1 + B1, \\min (C2 + x, 20) - A2 + B2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRentalEnv(DiscreteEnvironment):\n",
    "    \n",
    "    def poisson_prob(self, actual, mean):\n",
    "        # iterative, to keep the components from getting too large or small:\n",
    "        p = math.exp(-mean)\n",
    "        for i in range(0,actual):\n",
    "            p *= mean\n",
    "            p /= i+1\n",
    "        return p\n",
    "\n",
    "    def cum_non_poisson_prob(self, actual, mean):\n",
    "        prob = 1.0\n",
    "        for i in range(0,actual):\n",
    "            prob -= self.poisson_prob(i, mean)\n",
    "        return prob\n",
    "            \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Grid Size NxN\n",
    "        self.N = 21\n",
    "        \n",
    "        # States set S\n",
    "        self.states = [(i,j) for i in range(0,self.N) for j in range(0,self.N)]\n",
    "        \n",
    "        # Action sets for every state A(s)\n",
    "        # actions_superset = [-5,-4,-3,-2,-1,0,1,2,3,4,5]\n",
    "        self.actions ={}\n",
    "        for state in self.states:\n",
    "            C1 = state[0]\n",
    "            C2 = state[1]\n",
    "            low = max(-C2,-5)\n",
    "            high = min(C1,5)\n",
    "            self.actions[state] = range(low,high+1)\n",
    "        \n",
    "        # Discount factor Gamma\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        self.lam1 = 3.0\n",
    "        self.lam2 = 4.0\n",
    "        self.lam3 = 3.0\n",
    "        self.lam4 = 2.0\n",
    "\n",
    "        self.rewards = {}\n",
    "        self.transitions = np.zeros((self.N,self.N,self.N))\n",
    "        \n",
    "        for state in self.states:\n",
    "            C1 = state[0]\n",
    "            C2 = state[1]\n",
    "            action_list = self.actions[state]\n",
    "            for x in action_list:\n",
    "                \n",
    "                \n",
    "                A1 = min(np.random.poisson(lam=self.lam1), min(C1-x,20))\n",
    "                A2 = min(np.random.poisson(lam=self.lam2), min(C2+x,20))\n",
    "                B1 = min(np.random.poisson(lam=self.lam3), 20 - min(C1-x,20) + A1)\n",
    "                B2 = min(np.random.poisson(lam=self.lam4), 20 - min(C2+x,20) + A2)\n",
    "                \n",
    "                dest = (min(C1-x,20)-A1+B1, min(C2+x,20)-A2+B2)\n",
    "                tup = (state,x,dest)\n",
    "                \n",
    "                # Rewards r(s,a)\n",
    "                self.rewards[tup] = (10 *(A1 + A2)) - (2* abs(x))\n",
    "                \n",
    "                # Transition function p(s'|s,a) -> [s,a,s']\n",
    "                P1 = 0\n",
    "                if A1 < min(C1-x,20):\n",
    "                    P1 += self.poisson_prob(A1,self.lam1)\n",
    "                else:\n",
    "                    P1 += self.cum_non_poisson_prob(A1,self.lam1)\n",
    "                    \n",
    "                P2 = 0\n",
    "                if A2 < min(C2+x,20):\n",
    "                    P2 += self.poisson_prob(A2,self.lam2)\n",
    "                else:\n",
    "                    P2 += self.cum_non_poisson_prob(A2,self.lam2)\n",
    "                    \n",
    "                Q1 = 0\n",
    "                if B1 < 20 - min(C1-x,20) + A1:\n",
    "                    Q1 += self.poisson_prob(B1,self.lam3)\n",
    "                else:\n",
    "                    Q1 += self.cum_non_poisson_prob(B1,self.lam3)\n",
    "                    \n",
    "                Q2 = 0\n",
    "                if B2 < 20 - min(C2+x,20) + A2:\n",
    "                    Q2 += self.poisson_prob(B2,self.lam4)\n",
    "                else:\n",
    "                    Q2 += self.cum_non_poisson_prob(B2,self.lam4)\n",
    "                    \n",
    "                self.transitions[tup] = P1 * P2 * Q1 * Q2 * 1.0\n",
    "                    \n",
    "        # Current state of agent\n",
    "        self.agent_state = self.initial_state()\n",
    "        \n",
    "        # Has the game terminated?\n",
    "        self.is_terminated = False\n",
    "        \n",
    "        self.step_count = 0\n",
    "    \n",
    "    def step(self, x): #x is action\n",
    "        self.step_count += 1\n",
    "        state = self.agent_state            \n",
    "        dest_states = []\n",
    "        dest_probs = []\n",
    "        for dest in self.states:\n",
    "            dest_states.append(dest)\n",
    "            dest_probs.append(self.transitions[(state,x,dest)])\n",
    "        dest = np.random.choice(dest_states, dest_probs)\n",
    "        self.agent_state = dest\n",
    "        reward = self.rewards[(state,x,dest)]\n",
    "        if self.step_count >= 100:\n",
    "            self.is_terminated = True\n",
    "        return [self.agent_state, reward, self.is_terminated]\n",
    "         \n",
    "#         C1 = state[0]\n",
    "#         C2 = state[1]\n",
    "#         tempA1 = min(C1-x,20)\n",
    "#         tempA2 = min(C2+x,20)\n",
    "#         for poisA1 in range(0,tempA1+1):\n",
    "#             tempB1 = 20 - temp_A1 + poisA1\n",
    "#             for poisA2 in range(0,tempA2+1):\n",
    "#                 tempB2 = 20 - tempA2 + poisA2\n",
    "#                 for poisB1 in range(0,tempB1+1):\n",
    "#                     for poisB2 in range(0,tempB2+1):\n",
    "#                         dest = (tempA1-poisA1+poisB1,tempA2-poisA2+poisB2)\n",
    "#                         tup = (state,x,dest)\n",
    "#                         reward += self.transitions[tup] * self.rewards[tup]\n",
    "        \n",
    "    def reset(self): \n",
    "        self.agent_state = self.initial_state()\n",
    "        is_terminated = False\n",
    "        self.step_count = 0\n",
    "        return self.agent_state\n",
    "    \n",
    "    def initial_state(self):\n",
    "        return (self.N-1,self.N-1)\n",
    "    \n",
    "    def final_state(self):\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
