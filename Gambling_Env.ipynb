{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Imports.ipynb\n",
    "%run Discrete_Env.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gambling Environment\n",
    "\n",
    "### State Space $S$\n",
    "A state $s \\in S$ in this MDP indicates the gambler's capital or balance.  \n",
    "The state space $S = \\{1,2 ... 99\\}$  \n",
    "$|S| = 99$ states  \n",
    "This is an episodic finite MDP.\n",
    "\n",
    "### Action Space $A$\n",
    "The action $a \\in A$ performed by the gambler is to propose a stake.\n",
    "The action space for any state $s \\in S$ is:  \n",
    "$A(s) = \\{0,1,2 ... \\min(s,100-s)\\}$  \n",
    " \n",
    "### Flow from $(s(t)$, $a(t))$ to $s(t+1)$\n",
    "$s(t) = s$, the gambler's capital at time t    \n",
    "$a(t) = a$, the stakes proposed by the gambler   \n",
    "$a \\leq \\min(s,100-s)$  \n",
    " \n",
    "Internally:   \n",
    "The coin is biased to show heads on a coin toss with probability $p, 0 \\leq p \\leq 1$  \n",
    "So, with probability $p$, the gambler wins $a$ stakes, and with probability $1-p$ he loses $a$ stakes.\n",
    "\n",
    "### Transition Probability Function $p(s'|s,a)$\n",
    "The transition probability function for taking action $a(t) = a$ from state $s(t) = s$ to reach state $s(t+1)$ is described as follows:  \n",
    "Let $s(t) = s$  \n",
    "$a(t) = a$    \n",
    "$a \\leq \\min(s,100-s)$      \n",
    "Then $p(s+a \\ | \\ s, a) = p$   \n",
    "And $p(s-a \\ | \\ s, a) = 1-p$  \n",
    "For all other states $s'$, $p(s' \\ | \\ s,a) = 0$  \n",
    "  \n",
    "### Reward Function $R(s,a,s')$\n",
    "The reward is zero on all transitions except those on which the gambler reaches his goal, when it is $+1$.  \n",
    "$R(s,a,100) = +1$  \n",
    "For all other $s' \\neq 100$, $R(s,a,s') = 0$.\n",
    "  \n",
    "### Discount Rate $\\gamma$  \n",
    "Discount rate for this MDP is defined to be $\\gamma = 1.0 \\ (0 <= \\gamma <= 1)$.  \n",
    "\n",
    "## Bellman Updates for Value Iteration\n",
    "\n",
    "$\\forall \\ s \\in S$:  \n",
    "$V(s) = \\max_{a} \\sum_{s'} p(s'|s,a)[r(s,a,s') + \\gamma \\cdot V(s')]$  \n",
    "(Bellman Optimality Equation)    \n",
    "\n",
    "Similarly, we can formulate the Bellman Optimality Equation to get the improved state values:    \n",
    "$V(s) = \\max_{a} \\sum_{s'}  p(s'|s,a) [R(s,a,s') + 1.0 V(s')]$    \n",
    "$ = \\max_{a = 0}^{\\max(s,100-s)} \\{ p(s+a|s,a) [R(s,a,s+a) + V(s+a)] + p(s-a|s,a) [R(s,a,s-a) + V(s-a)]\\}$  \n",
    "$ = \\max_{a = 0}^{\\max(s,100-s)} \\{ p [R(s,a,s+a) + V(s+a)] + (1-p) [R(s,a,s-a) + V(s-a)]\\}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamblingEnv(DiscreteEnvironment):\n",
    "    \n",
    "    def __init__(self,bias):\n",
    "        \n",
    "        # States set S\n",
    "        self.states = [i for i in range(1,100)]\n",
    "        \n",
    "        # Action sets for every state A(s)\n",
    "        self.actions = {s:[j for j in range(0,min(s,100-s))] for s in self.states}\n",
    "\n",
    "        # Discount factor Gamma\n",
    "        self.gamma = 1.0\n",
    "\n",
    "        # Bias of coin used\n",
    "        self.bias = bias\n",
    "        self.rewards = {}\n",
    "        self.transitions = {}\n",
    "        \n",
    "        for s in self.states:\n",
    "            action_list = self.actions[s]\n",
    "            for a in action_list:\n",
    "                for dest in self.states:\n",
    "                    self.rewards[(s,a,dest)] = 0\n",
    "                \n",
    "                tup_win = (s,a,s+a)\n",
    "                tup_loss = (s,a,s-a)\n",
    "                \n",
    "                # Transition function p(s'|s,a) -> [s,a,s']\n",
    "                self.transitions[tup_win] = bias\n",
    "                self.transitions[tup_loss] = 1 - bias\n",
    "                \n",
    "                # Rewards r(s,a,s')\n",
    "                if s+a == 100:\n",
    "                    self.rewards[tup_win] = 1000.0\n",
    "                else: \n",
    "                    self.rewards[tup_win] = 0.0\n",
    "                if s-a == 100:\n",
    "                    self.rewards[tup_loss] = 1.0\n",
    "                else: \n",
    "                    self.rewards[tup_loss] = 0.0\n",
    "                    \n",
    "        # Current state of agent\n",
    "        self.agent_state = self.initial_state()\n",
    "        \n",
    "        # Has the game terminated?\n",
    "        self.is_terminated = False\n",
    "        \n",
    "        self.step_count = 0\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.step_count += 1\n",
    "        s = self.agent_state\n",
    "        dest_states = [s+a,s-a]\n",
    "        dest_probs = [bias,1-bias]\n",
    "        dest = np.random.choice(dest_states, dest_probs)\n",
    "        self.agent_state = dest\n",
    "        reward = self.rewards[(state,x,dest)]\n",
    "        if self.is_final_state(self.agent_state):\n",
    "            self.is_terminated = True\n",
    "        return [self.agent_state, reward, self.is_terminated]\n",
    "        \n",
    "    def reset(self): \n",
    "        self.agent_state = self.initial_state()\n",
    "        is_terminated = False\n",
    "        self.step_count = 0\n",
    "        return self.agent_state\n",
    "    \n",
    "    def initial_state(self):\n",
    "        return random.choice(self.states)\n",
    "    \n",
    "    def is_final_state(self,s):\n",
    "        if s == 0 or s == 100:\n",
    "            return 1\n",
    "        return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
